{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading ad clusters and image SHA1s\n",
    "\n",
    "Loading cluster, ad and SHA1 ships from the base file.\n",
    "\n",
    "## Expected files\n",
    "- ``compute_many_descriptors`` script output CSV of images that were actually processed\n",
    "- CSV mapping association of [clusterID, ad, SHA1]\n",
    "\n",
    "## Filter based on computed descriptors.\n",
    "Some images were not valid (i.e. were gifs or HTML pages), and should not be considered.\n",
    "This may cause some ads to no longer have child images and thus should also be not considered.\n",
    "This again applies to clusters that have no resulting child ads.\n",
    "\n",
    "Maps and files saved from this point on have been filtered by what has been actually computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__depends__ = ['map.cp1_data.csv']\n",
    "__dest__ = [\n",
    "    'positive.cluster2ads.pickle',\n",
    "    'positive.cluster2shas.pickle',\n",
    "    'positive.ad2shas.pickle',\n",
    "    'positive.sha2ads.pickle',\n",
    "    'negative.cluster2ads.pickle',\n",
    "    'negative.cluster2shas.pickle',\n",
    "    'negative.ad2shas.pickle',\n",
    "    'negative.sha2ads.pickle',\n",
    "    'train_pos_clusters.pickle',\n",
    "    'train_pos_ads.pickle',\n",
    "    'train_pos_shas.pickle',\n",
    "    'train_neg_clusters.pickle',\n",
    "    'train_neg_ads.pickle',\n",
    "    'train_neg_shas.pickle',\n",
    "    'test_pos_clusters.pickle',\n",
    "    'test_pos_ads.pickle',\n",
    "    'test_pos_shas.pickle',\n",
    "    'test_neg_clusters.pickle',\n",
    "    'test_neg_ads.pickle',\n",
    "    'test_neg_shas.pickle',\n",
    "    'test_eval_gt.jl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster/ad/SHA1/label relationship for each image to be considered\n",
    "CP1_DATA_CSV = 'map.cp1_data.csv'\n",
    "# Output files\n",
    "# - positive/negative sub-maps\n",
    "POS_CLUSTER_ID_TO_AD_IDS = 'positive.cluster2ads.pickle'\n",
    "POS_CLUSTER_ID_TO_SHAS   = 'positive.cluster2shas.pickle'\n",
    "POS_AD_ID_TO_SHAS        = 'positive.ad2shas.pickle'\n",
    "POS_SHA_TO_AD_IDS        = 'positive.sha2ads.pickle'\n",
    "NEG_CLUSTER_ID_TO_AD_IDS = 'negative.cluster2ads.pickle'\n",
    "NEG_CLUSTER_ID_TO_SHAS   = 'negative.cluster2shas.pickle'\n",
    "NEG_AD_ID_TO_SHAS        = 'negative.ad2shas.pickle'\n",
    "NEG_SHA_TO_AD_IDS        = 'negative.sha2ads.pickle'\n",
    "# - positive/negative ID sets\n",
    "TRAIN_POS_CLUSTER_IDS = 'train_pos_clusters.pickle'\n",
    "TRAIN_POS_AD_IDS      = 'train_pos_ads.pickle'\n",
    "TRAIN_POS_SHA1S       = 'train_pos_shas.pickle'\n",
    "TRAIN_NEG_CLUSTER_IDS = 'train_neg_clusters.pickle'\n",
    "TRAIN_NEG_AD_IDS      = 'train_neg_ads.pickle'\n",
    "TRAIN_NEG_SHA1S       = 'train_neg_shas.pickle'\n",
    "TEST_POS_CLUSTER_IDS  = 'test_pos_clusters.pickle'\n",
    "TEST_POS_AD_IDS       = 'test_pos_ads.pickle'\n",
    "TEST_POS_SHA1S        = 'test_pos_shas.pickle'\n",
    "TEST_NEG_CLUSTER_IDS  = 'test_neg_clusters.pickle'\n",
    "TEST_NEG_AD_IDS       = 'test_neg_ads.pickle'\n",
    "TEST_NEG_SHA1S        = 'test_neg_shas.pickle'\n",
    "#\n",
    "TEST_EVAL_GT_JL = 'test_eval_gt.jl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos_computed_shas = set()\n",
    "neg_computed_shas = set()\n",
    "\n",
    "pos_cluster2ads = collections.defaultdict(set)\n",
    "pos_cluster2shas = collections.defaultdict(set)\n",
    "pos_ad2shas = collections.defaultdict(set)\n",
    "pos_sha2ads = collections.defaultdict(set)\n",
    "\n",
    "neg_cluster2ads = collections.defaultdict(set)\n",
    "neg_cluster2shas = collections.defaultdict(set)\n",
    "neg_ad2shas = collections.defaultdict(set)\n",
    "neg_sha2ads = collections.defaultdict(set)\n",
    "\n",
    "# SHA1 values of images actually computable\n",
    "print \"Loading cp1_data csv\"\n",
    "with open(CP1_DATA_CSV) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for r in reader:\n",
    "        c_id, ad_id, sha, label = r\n",
    "        label = int(label)\n",
    "\n",
    "        if label == 1:  # positive\n",
    "            pos_computed_shas.add(sha)\n",
    "            pos_cluster2ads[c_id].add(ad_id)\n",
    "            pos_cluster2shas[c_id].add(sha)\n",
    "            pos_ad2shas[ad_id].add(sha)\n",
    "            pos_sha2ads[sha].add(ad_id)\n",
    "        elif label == 0:\n",
    "            neg_computed_shas.add(sha)\n",
    "            neg_cluster2ads[c_id].add(ad_id)\n",
    "            neg_cluster2shas[c_id].add(sha)\n",
    "            neg_ad2shas[ad_id].add(sha)\n",
    "            neg_sha2ads[sha].add(ad_id)\n",
    "        else:\n",
    "            raise ValueError(\"Got unexpected truth label: %s\" % label)\n",
    "        \n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pos_computed_shas), len(neg_computed_shas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that the negative example cluster IDs are distict from postive example cluster IDs\n",
    "#\n",
    "# If negative cluster IDs intersect positive cluster IDs,\n",
    "# re-assign negative cluster IDs by increasing by max(pos_cluster_ids)\n",
    "pos_cluster_ids = set(pos_cluster2ads)\n",
    "neg_cluster_ids = set(neg_cluster2ads)\n",
    "# if there is intersection....\n",
    "if pos_cluster_ids & neg_cluster_ids:\n",
    "    print \"Reassigning cluster IDs\"\n",
    "    offset = max(pos_cluster_ids)\n",
    "    new_neg_cluster2ads  = collections.defaultdict(set)\n",
    "    new_neg_cluster2shas = collections.defaultdict(set)\n",
    "    \n",
    "    neg_cluster_id_old2new = {}\n",
    "    \n",
    "    for cid in sorted(neg_cluster_ids, reverse=True):\n",
    "        print \"- %d -> %d\" % (cid, cid+offset)\n",
    "        neg_cluster_id_old2new[cid] = cid+offset\n",
    "        \n",
    "        new_neg_cluster2ads[cid+offset] = neg_cluster2ads[cid]\n",
    "        new_neg_cluster2shas[cid+offset] = neg_cluster2shas[cid]\n",
    "    \n",
    "    neg_cluster_ids = set(new_neg_cluster2ad)\n",
    "    neg_cluster2ads = new_neg_cluster2ads\n",
    "    neg_cluster2shas = new_neg_cluster2shas\n",
    "    del new_neg_cluster2ads, new_neg_cluster2shas\n",
    "    \n",
    "    with open('negative.cluster_id_reassignment.old2new.pickle', 'w') as f:\n",
    "        print \"Saving reassignment mapping\"\n",
    "        pickle.dump(neg_cluster_id_old2new, f, -1)\n",
    "    print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SHA1's collected should now be <= to the SHA1s computed\n",
    "print len( {s for c, shas in pos_cluster2shas.iteritems() for s in shas}.difference(pos_computed_shas) )\n",
    "print len( {s for c, shas in neg_cluster2shas.iteritems() for s in shas}.difference(neg_computed_shas) )\n",
    "\n",
    "# Number of intersectring SHA1 between positive and negative set\n",
    "print len(set(pos_sha2ads) & set(neg_sha2ads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving clusters\n",
    "import json\n",
    "\n",
    "def convert_dict(a):\n",
    "    return dict( (k, list(v)) for k, v in a.iteritems() )\n",
    "\n",
    "def pickle_dump(obj, fp):\n",
    "    with open(fp, 'wb') as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "\n",
    "json_params = {\"indent\": 2, \"separators\": (',', ': '), \"sort_keys\": True}\n",
    "\n",
    "\n",
    "# Saving positive info\n",
    "print \"Saving POS cluster->ads\"\n",
    "pickle_dump(pos_cluster2ads, POS_CLUSTER_ID_TO_AD_IDS)\n",
    "\n",
    "print \"Saving POS cluster->image shas\"\n",
    "pickle_dump(pos_cluster2shas, POS_CLUSTER_ID_TO_SHAS)\n",
    "\n",
    "print \"Saving POS ad->image shas\"\n",
    "pickle_dump(pos_ad2shas, POS_AD_ID_TO_SHAS)\n",
    "    \n",
    "print \"Saving POS SHA1->ads\"\n",
    "pickle_dump(pos_sha2ads, POS_SHA_TO_AD_IDS)\n",
    "\n",
    "\n",
    "# Saving negative info\n",
    "print \"Saving NEG cluster->ads\"\n",
    "pickle_dump(neg_cluster2ads, NEG_CLUSTER_ID_TO_AD_IDS)\n",
    "\n",
    "print \"Saving NEG cluster->image shas\"\n",
    "pickle_dump(neg_cluster2shas, NEG_CLUSTER_ID_TO_SHAS)\n",
    "\n",
    "print \"Saving NEG ad->image shas\"\n",
    "pickle_dump(neg_ad2shas, NEG_AD_ID_TO_SHAS)\n",
    "\n",
    "print \"Saving NEG SHA1->ads\"\n",
    "pickle_dump(neg_sha2ads, NEG_SHA_TO_AD_IDS)\n",
    "\n",
    "\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating data based on Clusters\n",
    "\n",
    "Based on ordering clusters by the total number of child images.  This lets us base train/test sets of approximately the same relative sizes.\n",
    "\n",
    "Train has so many images because one cluster has ~30k child images.\n",
    "\n",
    "We create a \"train\" set set with ~75% of images and a \"test\" set with the remaining ~25% of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_clusters_ordered = sorted( pos_cluster2shas, \n",
    "                               key=lambda c: ( len(pos_cluster2shas[c]), c ),\n",
    "                               reverse=1 )\n",
    "neg_clusters_ordered = sorted( neg_cluster2shas,\n",
    "                               key=lambda c: ( len(neg_cluster2shas[c]), c ),\n",
    "                               reverse=1)\n",
    "\n",
    "split_int = 5\n",
    "test_split = split_int - 1\n",
    "\n",
    "# Image classifier training clusters/ads/shas\n",
    "train_pos_clusters = { c   for i, c in enumerate(pos_clusters_ordered) if i % split_int != test_split }\n",
    "train_neg_clusters = { c   for i, c in enumerate(neg_clusters_ordered) if i % split_int != test_split }\n",
    "train_pos_ads      = { ad  for c in train_pos_clusters for ad  in pos_cluster2ads[c] }\n",
    "train_neg_ads      = { ad  for c in train_neg_clusters for ad  in neg_cluster2ads[c] }\n",
    "train_pos_shas     = { sha for c in train_pos_clusters for sha in pos_cluster2shas[c] }\n",
    "train_neg_shas     = { sha for c in train_neg_clusters for sha in neg_cluster2shas[c] }\n",
    "\n",
    "# Test/Validation clusters/ads/shas\n",
    "test_pos_clusters   = { c for i, c in enumerate(pos_clusters_ordered) if i % split_int == test_split }\n",
    "test_neg_clusters   = { c for i, c in enumerate(neg_clusters_ordered) if i % split_int == test_split }\n",
    "test_pos_ads        = { ad  for c in test_pos_clusters for ad  in pos_cluster2ads[c] }\n",
    "test_neg_ads        = { ad  for c in test_neg_clusters for ad  in neg_cluster2ads[c] }\n",
    "test_pos_shas       = { sha for c in test_pos_clusters for sha in pos_cluster2shas[c] }\n",
    "test_neg_shas       = { sha for c in test_neg_clusters for sha in neg_cluster2shas[c] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Train 1 (image)\"\n",
    "print \"  (pos)| clusters:\", len(train_pos_clusters)\n",
    "print \"       | ads:\",      len(train_pos_ads)\n",
    "print \"       | images:\",   len(train_pos_shas)\n",
    "print\n",
    "print \"  (neg)| clusters:\", len(train_neg_clusters)\n",
    "print \"       | ads:\",      len(train_neg_ads)\n",
    "print \"       | images:\",   len(train_neg_shas)\n",
    "print\n",
    "print \"Test\"\n",
    "print \"  (pos)| clusters:\", len(test_pos_clusters)\n",
    "print \"       | ads:\",      len(test_pos_ads)\n",
    "print \"       | images:\",   len(test_pos_shas)\n",
    "print\n",
    "print \"  (neg)| clusters:\", len(test_neg_clusters)\n",
    "print \"       | ads:\",      len(test_neg_ads)\n",
    "print \"       | images:\",   len(test_neg_shas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train - for image classifier\n",
    "pickle_dump(train_pos_clusters, TRAIN_POS_CLUSTER_IDS)\n",
    "pickle_dump(train_pos_ads, TRAIN_POS_AD_IDS)\n",
    "pickle_dump(train_pos_shas, TRAIN_POS_SHA1S)\n",
    "\n",
    "pickle_dump(train_neg_clusters, TRAIN_NEG_CLUSTER_IDS)\n",
    "pickle_dump(train_neg_ads, TRAIN_NEG_AD_IDS)\n",
    "pickle_dump(train_neg_shas, TRAIN_NEG_SHA1S)\n",
    "\n",
    "# Test - for image/ad/cluster classifier validation\n",
    "pickle_dump(test_pos_clusters, TEST_POS_CLUSTER_IDS)\n",
    "pickle_dump(test_pos_ads, TEST_POS_AD_IDS)\n",
    "pickle_dump(test_pos_shas, TEST_POS_SHA1S)\n",
    "\n",
    "pickle_dump(test_neg_clusters, TEST_NEG_CLUSTER_IDS)\n",
    "pickle_dump(test_neg_ads, TEST_NEG_AD_IDS)\n",
    "pickle_dump(test_neg_shas, TEST_NEG_SHA1S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating grount-truth json-lines file for test-set for use with MEMEX-provided evaluation script\n",
    "# format: {\"cluster_id\": \"<number>\", \"class\": <int>}\n",
    "# Class value should be:\n",
    "# - 1 for positive\n",
    "# - 0 for negative\n",
    "with open(TEST_EVAL_GT_JL, 'w') as f:\n",
    "    for c in sorted(pos_cluster_ids):\n",
    "        f.write( json.dumps({\"cluster_id\": str(c), \"class\": 1}) + \"\\n\" )\n",
    "    for c in sorted(neg_cluster_ids):\n",
    "        f.write( json.dumps({\"cluster_id\": str(c), \"class\": 0}) + \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHA1 Intersection Investigation\n",
    "\n",
    "Some images have been found to be shared across ads in different clusters.  Since clusters are supposed to represent distictly seperate entities or relationships, this shows that either the clusters are not linkable via multimedia only, they were incorrectly clustered, or actively split up on purpose.  For the purpose of our approach (image-base classification), this means that the same images will potentially show up in both or all of the train/test/evaluation data sets.\n",
    "\n",
    "Traditionally, the presence of the same/similar images in both train and test sets leads to faulty evaluation because the classifier has an easier time handling data it was trained on, and thus artificially higher scores.  This is the same here in that train/test scores will probably be higher with their presence on both sides.  However, if their shared presence is a strong positive indicator of a new HT ad, so their repeated positive recognition is a boon.  On the other hand, again, we may want to see/measure how the classifier is performing due to abstract features, not including repeat imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test_intersection = train_pos_shas & test_pos_shas\n",
    "\n",
    "print len(train_test_intersection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
